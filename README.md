## Towards Effective Data-Free Knowledge Distillation via Diverse Diffusion Augmentation

MM2024 paper: Towards Effective Data-Free Knowledge Distillation via Diverse Diffusion Augmentation.

Author: Muquan Li,  Dongyang Zhang,  Tao He,  Xiurui Xie,  Yuan-Fang Li,  Ke Qin

<img src="images/overview.png">

### 1. Environment

This repository is tested with Ubuntu 18.04.5 LTS, python 3.6.5, pytorch 1.7.1 and cuda 11.4.

### 2. Get the pre-trained teacher model

```
python train_scratch.py --model wrn40_2 --dataset cifar10
```

After the training is completed, the teacher model will be saved as `checkpoints/pretrained/cifar10_wrn40_2.pth`.

Or you can directly download pre-trained teacher models from [Dropbox-Models (266 MB)](https://www.dropbox.com/sh/w8xehuk7debnka3/AABhoazFReE_5mMeyvb4iUWoa?dl=0) and extract them as `checkpoints/pretrained`.

### 3. Initialize image bank

To prevent the student from overfitting to data generated by early training rounds, it is necessary to synthesize some data firstly to initialize image bank by removing $\mathcal{L}_{csd}$ , i.e., setting the flag `--csd` to `0`, and running 400 synthesis batches with each one containing 200 samples.

```
bash scripts/csd/csd_cifar10_initBank_wrn402.sh
```

### 3. Diffusion Augmentation

Augment images through [Stable Diffusion-V2](https://huggingface.co/spaces/lambdalabs/stable-diffusion-image-variations)

### 4. Train student with DDA

```
bash scripts/dda/dda_cifar10_wrn402_wrn161.sh
```

### 5. Train student with comparison methods

```
bash scripts/xxx/xxx.sh # e.g. scripts/zskt/zskt_cifar10_wrn402_wrn161.sh
```



